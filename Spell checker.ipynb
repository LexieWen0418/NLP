{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction Tool\n",
    "- By spelling correction, we mean predicting the correct word from an incorrectly typed word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The steps in spell checking:__\n",
    "- Read in a large corpus of words\n",
    "- Count the number of times of appearance of each word\n",
    "- Generate candidate words from the input word that are -:\n",
    "    - Input word itself\n",
    "    - Words that are 1 edit distance (by way of insert, delete, transpose, replace) away\n",
    "    - Words that are 2 edit distance (by way of insert, delete, transpose, replace) away\n",
    "- Find the candidate word with the maximum probability of occurring in the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Building word list](#Building-word-list)\n",
    "\n",
    "2. [Spelling Correction](#Spelling-Correction)\n",
    "\n",
    "3. [Test](#Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/lexiew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SOS_DB', 'docdx_db_production']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymysql\n",
    "import psycopg2\n",
    "import configparser\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import inflect as inf\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "config.sections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building word list\n",
    "__STEPS__\n",
    "- Fetch all Answer data from docdx\n",
    "- Cleaning data \n",
    "    - messy code\n",
    "    - puntuation\n",
    "    - singular/plural\n",
    "    - white/double space\n",
    "    - digit\n",
    "    - stopwords\n",
    "- store into a file (txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_db_section = config['docdx_db_production']\n",
    "host = import_db_section['host']\n",
    "user = import_db_section['user']\n",
    "password = import_db_section['password']\n",
    "db_dx=import_db_section['db']\n",
    "port = int(import_db_section.get('port',5432))\n",
    "\n",
    "conn_str = \"host={} dbname={} user={} password={}\".format(host, db_dx, user, password)\n",
    "db_docdx = psycopg2.connect(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data + data cleaning \n",
    "def get_comment():\n",
    "    query_comment = \"\"\"select id comment_id, topic_id, created_by, text from comment\n",
    "                        where reply_to_comment_id is null and modified_by is null and deleted_at is null\n",
    "                        order by created_at \"\"\"\n",
    "    comment = pd.read_sql(query_comment, db_docdx)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = get_comment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "def remove_punc(x):\n",
    "    exclude = set(\",.:;'\\\"-?!/{}()+%&\")\n",
    "    return \"\".join([(ch if ch not in exclude else \" \") for ch in x])\n",
    "\n",
    "def data_cleaning(df):\n",
    "    #remove messy code\n",
    "    p = inf.engine()\n",
    "    #p = inflect.engine()\n",
    "    #p.singular_noun('apples')\n",
    "    df['text'] = df.text.apply(lambda x: re.sub('\\n', \" \", x))\n",
    "    df['text'] = df.text.apply(lambda x: re.sub('\\t', \" \", x))\n",
    "    df['text'] = df.text.apply(lambda x: re.sub('w/', \"with\", x))\n",
    "    \n",
    "    #remove punc\n",
    "    df['cleaned_text'] = df['text'].apply(lambda x: remove_punc(x)).str.lower()\n",
    "    \n",
    "    #remove digit\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    \n",
    "    #remove double space\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub('\\s+',' ', x))\n",
    "    #remove stopwords\n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.\\\n",
    "                                                  join([word for word in x.split(' ') if word not in stop_words]))\n",
    "    \n",
    "    df = df[df.cleaned_text != '']\n",
    "    df = df[df.cleaned_text != ' ']\n",
    "    \n",
    "    #plural to singular \n",
    "    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([word if p.singular_noun(word) is \n",
    "                                                                     False else p.singular_noun(word) for word in x.split(' ')]))\n",
    "    return df\n",
    "\n",
    "#value_when_true if condition else value_when_false\n",
    "#[x+1 if x >= 45 else x+5 for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = data_cleaning(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>created_by</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>560652</td>\n",
       "      <td>In patients wth moderate to severe IBD, biolog...</td>\n",
       "      <td>patient wth moderate severe ibd biologic clear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>563658</td>\n",
       "      <td>The problem with stopping Biologics is the po...</td>\n",
       "      <td>problem stopping biologic potential formation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>522329</td>\n",
       "      <td>Yes they can- I've had many patient achieve re...</td>\n",
       "      <td>ye many patient achieve remission using biolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>585883</td>\n",
       "      <td>The risk is developing HACA antibodies against...</td>\n",
       "      <td>risk developing haca antibody biologic trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>568457</td>\n",
       "      <td>If biologics were not antigenic (inducing neut...</td>\n",
       "      <td>biologic antigenic inducing neutralizing antib...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  topic_id  created_by  \\\n",
       "0          24         2      560652   \n",
       "1          25         2      563658   \n",
       "2          26         2      522329   \n",
       "3          27         2      585883   \n",
       "4          28         2      568457   \n",
       "\n",
       "                                                text  \\\n",
       "0  In patients wth moderate to severe IBD, biolog...   \n",
       "1   The problem with stopping Biologics is the po...   \n",
       "2  Yes they can- I've had many patient achieve re...   \n",
       "3  The risk is developing HACA antibodies against...   \n",
       "4  If biologics were not antigenic (inducing neut...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  patient wth moderate severe ibd biologic clear...  \n",
       "1   problem stopping biologic potential formation...  \n",
       "2  ye many patient achieve remission using biolog...  \n",
       "3  risk developing haca antibody biologic trainin...  \n",
       "4  biologic antigenic inducing neutralizing antib...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all words in a file \n",
    "def store_file(df):\n",
    "    text = ' '.join(i for i in df.cleaned_text.str.lower())\n",
    "    #text = ' '.join(i for i in df_comment.text.str.lower())\n",
    "    file = open(\"comment_up_to_date.txt\", \"w\")\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "store_file(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n",
    "\n",
    "\n",
    "- The input word is first split into possible pairs of words. \n",
    "- A set of candidate words are generated from these pairs by performing deletion, transposition, replacement, insertion at edit distance 1 and edit distance 2. \n",
    "- The candidates are then checked for their presence in the corpus and that word is chosen which has the maximum probability of occurrance in the corpus. \n",
    "    - (Please note however, that the input word is preferred to candidate words at edit distance 1 which in turn is preferred over candidates at edit distance 2 away.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def known(words): return set(w for w in words if w in WORDS)\n",
    "\n",
    "def correction(word):\n",
    "    \"Find the best spelling correction for word.\"\n",
    "    candidates = (known([word]) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=WORDS.get)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "The following is just a minor modification to preserve the case and punctuation in a sentence \n",
    "while performing the spelling correction for each word comprising the sentence.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def correct_text(text):\n",
    "    \"Correct all the words within a text, returning the corrected text.\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "def correct_match(match):\n",
    "    \"Spell-correct word in match, and preserve proper upper/lower/title case.\"\n",
    "    word = match.group()\n",
    "    return case_of(word)(correction(word.lower()))\n",
    "\n",
    "def case_of(text):\n",
    "    \"Return the case-function appropriate for text: upper, lower, title, or just str.\"\n",
    "    return (str.upper if text.isupper() else\n",
    "            str.lower if text.islower() else\n",
    "            str.title if text.istitle() else\n",
    "            str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in docdx: 22403 \n"
     ]
    }
   ],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "WORDS = Counter(words(open(r'comment_up_to_date.txt', encoding = \"ISO-8859-1\").read()))\n",
    "\n",
    "\n",
    "print(\"Total number of unique words in docdx: %d \" % len(WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biopsy biopsy biopsy'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text(\"\"\"biopsy biosy biossy\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stopped biologic patient los efficacy develop intolerance drug'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('stopped biologic patiens los efficay develop intolerance drug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'migraine migraine'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text(\"\"\"migraine migrainne\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cancer cancer cancer'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text(\"\"\"cancer canner canccer\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
